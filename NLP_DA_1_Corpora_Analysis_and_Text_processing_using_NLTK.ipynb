{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathiyanarayangs/Corpora-Analysis-and-Text-Processing-using-NLTK/blob/main/NLP_DA_1_Corpora_Analysis_and_Text_processing_using_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP DA - 1 : Corpora Analysis and Text processing using NLTK**\n",
        "\n",
        "**Done By: Sathiyanarayan GS - 20BCE1045** \n",
        "\n",
        "\n",
        "1. Utilize Python NLTK (Natural Language Tool Kit) Platform and do the following. Install relevant Packages and Libraries \n",
        "\n",
        "• Explore Brown Corpus and find the size, tokens, categories,\n",
        "\n",
        "• Find the size of word tokens?\n",
        "\n",
        "• Find the size of word types?\n",
        "\n",
        "• Find the size of the category “government”\n",
        "\n",
        "• List the most frequent tokens\n",
        "\n",
        "• Count the number of sentences"
      ],
      "metadata": {
        "id": "ghu0ZrzIZTdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnmWZewoBfnx",
        "outputId": "112cc42e-7411-46cb-b4e4-afdaafc8301f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-yVG2eJJxin",
        "outputId": "c3bf25c4-74fb-404c-fce8-2f6306363190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "CAgzFtrKdYr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of brown corpus\n",
        "corpus_size = len(brown.words())\n",
        "print(\"Corpus size:\", corpus_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uSSHBzLcRLl",
        "outputId": "44f3e095-9c1b-4220-c547-638412fad308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus size: 1161192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens in brown corpus\n",
        "tokens = nltk.word_tokenize(brown.raw())\n",
        "print(\"Number of Tokens:\", len(tokens))\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYtO7i6_cXSs",
        "outputId": "4f7d2e8e-8b36-44fc-fb94-9e0b44485376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tokens: 1439319\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The/at',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " 'Grand/jj-tl',\n",
              " 'Jury/nn-tl',\n",
              " 'said/vbd',\n",
              " 'Friday/nr',\n",
              " 'an/at',\n",
              " 'investigation/nn',\n",
              " 'of/in',\n",
              " \"Atlanta's/np\",\n",
              " '$',\n",
              " 'recent/jj',\n",
              " 'primary/nn',\n",
              " 'election/nn',\n",
              " 'produced/vbd',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'no/at',\n",
              " 'evidence/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'that/cs',\n",
              " 'any/dti',\n",
              " 'irregularities/nns',\n",
              " 'took/vbd',\n",
              " 'place/nn',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'further/rbr',\n",
              " 'said/vbd',\n",
              " 'in/in',\n",
              " 'term-end/nn',\n",
              " 'presentments/nns',\n",
              " 'that/cs',\n",
              " 'the/at',\n",
              " 'City/nn-tl',\n",
              " 'Executive/jj-tl',\n",
              " 'Committee/nn-tl',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'which/wdt',\n",
              " 'had/hvd',\n",
              " 'over-all/jj',\n",
              " 'charge/nn',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'election/nn',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'deserves/vbz',\n",
              " 'the/at',\n",
              " 'praise/nn',\n",
              " 'and/cc',\n",
              " 'thanks/nns',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'City/nn-tl',\n",
              " 'of/in-tl',\n",
              " 'Atlanta/np-tl',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'for/in',\n",
              " 'the/at',\n",
              " 'manner/nn',\n",
              " 'in/in',\n",
              " 'which/wdt',\n",
              " 'the/at',\n",
              " 'election/nn',\n",
              " 'was/bedz',\n",
              " 'conducted/vbn',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'September-October/np',\n",
              " 'term/nn',\n",
              " 'jury/nn',\n",
              " 'had/hvd',\n",
              " 'been/ben',\n",
              " 'charged/vbn',\n",
              " 'by/in',\n",
              " 'Fulton/np-tl',\n",
              " 'Superior/jj-tl',\n",
              " 'Court/nn-tl',\n",
              " 'Judge/nn-tl',\n",
              " 'Durwood/np',\n",
              " 'Pye/np',\n",
              " 'to/to',\n",
              " 'investigate/vb',\n",
              " 'reports/nns',\n",
              " 'of/in',\n",
              " 'possible/jj',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'irregularities/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'hard-fought/jj',\n",
              " 'primary/nn',\n",
              " 'which/wdt',\n",
              " 'was/bedz',\n",
              " 'won/vbn',\n",
              " 'by/in',\n",
              " 'Mayor-nominate/nn-tl',\n",
              " 'Ivan/np',\n",
              " 'Allen/np',\n",
              " 'Jr./np',\n",
              " './',\n",
              " '.',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'Only/rb',\n",
              " 'a/at',\n",
              " 'relative/jj',\n",
              " 'handful/nn',\n",
              " 'of/in',\n",
              " 'such/jj',\n",
              " 'reports/nns',\n",
              " 'was/bedz',\n",
              " 'received/vbn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'considering/in',\n",
              " 'the/at',\n",
              " 'widespread/jj',\n",
              " 'interest/nn',\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'election/nn',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'number/nn',\n",
              " 'of/in',\n",
              " 'voters/nns',\n",
              " 'and/cc',\n",
              " 'the/at',\n",
              " 'size/nn',\n",
              " 'of/in',\n",
              " 'this/dt',\n",
              " 'city/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " 'it/pps',\n",
              " 'did/dod',\n",
              " 'find/vb',\n",
              " 'that/cs',\n",
              " 'many/ap',\n",
              " 'of/in',\n",
              " \"Georgia's/np\",\n",
              " '$',\n",
              " 'registration/nn',\n",
              " 'and/cc',\n",
              " 'election/nn',\n",
              " 'laws/nns',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'are/ber',\n",
              " 'outmoded/jj',\n",
              " 'or/cc',\n",
              " 'inadequate/jj',\n",
              " 'and/cc',\n",
              " 'often/rb',\n",
              " 'ambiguous/jj',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'It/pps',\n",
              " 'recommended/vbd',\n",
              " 'that/cs',\n",
              " 'Fulton/np',\n",
              " 'legislators/nns',\n",
              " 'act/vb',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'to/to',\n",
              " 'have/hv',\n",
              " 'these/dts',\n",
              " 'laws/nns',\n",
              " 'studied/vbn',\n",
              " 'and/cc',\n",
              " 'revised/vbn',\n",
              " 'to/in',\n",
              " 'the/at',\n",
              " 'end/nn',\n",
              " 'of/in',\n",
              " 'modernizing/vbg',\n",
              " 'and/cc',\n",
              " 'improving/vbg',\n",
              " 'them/ppo',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'grand/jj',\n",
              " 'jury/nn',\n",
              " 'commented/vbd',\n",
              " 'on/in',\n",
              " 'a/at',\n",
              " 'number/nn',\n",
              " 'of/in',\n",
              " 'other/ap',\n",
              " 'topics/nns',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'among/in',\n",
              " 'them/ppo',\n",
              " 'the/at',\n",
              " 'Atlanta/np',\n",
              " 'and/cc',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " 'purchasing/vbg',\n",
              " 'departments/nns',\n",
              " 'which/wdt',\n",
              " 'it/pps',\n",
              " 'said/vbd',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'are/ber',\n",
              " 'well/ql',\n",
              " 'operated/vbn',\n",
              " 'and/cc',\n",
              " 'follow/vb',\n",
              " 'generally/rb',\n",
              " 'accepted/vbn',\n",
              " 'practices/nns',\n",
              " 'which/wdt',\n",
              " 'inure/vb',\n",
              " 'to/in',\n",
              " 'the/at',\n",
              " 'best/jjt',\n",
              " 'interest/nn',\n",
              " 'of/in',\n",
              " 'both/abx',\n",
              " 'governments/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'Merger/nn-hl',\n",
              " 'proposed/vbn-hl',\n",
              " 'However/wrb',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " 'it/pps',\n",
              " 'believes/vbz',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'these/dts',\n",
              " 'two/cd',\n",
              " 'offices/nns',\n",
              " 'should/md',\n",
              " 'be/be',\n",
              " 'combined/vbn',\n",
              " 'to/to',\n",
              " 'achieve/vb',\n",
              " 'greater/jjr',\n",
              " 'efficiency/nn',\n",
              " 'and/cc',\n",
              " 'reduce/vb',\n",
              " 'the/at',\n",
              " 'cost/nn',\n",
              " 'of/in',\n",
              " 'administration/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'City/nn-tl',\n",
              " 'Purchasing/vbg-tl',\n",
              " 'Department/nn-tl',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'is/bez',\n",
              " 'lacking/vbg',\n",
              " 'in/in',\n",
              " 'experienced/vbn',\n",
              " 'clerical/jj',\n",
              " 'personnel/nns',\n",
              " 'as/cs',\n",
              " 'a/at',\n",
              " 'result/nn',\n",
              " 'of/in',\n",
              " 'city/nn',\n",
              " 'personnel/nns',\n",
              " 'policies/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'It/pps',\n",
              " 'urged/vbd',\n",
              " 'that/cs',\n",
              " 'the/at',\n",
              " 'city/nn',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'take/vb',\n",
              " 'steps/nns',\n",
              " 'to/to',\n",
              " 'remedy/vb',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'this/dt',\n",
              " 'problem/nn',\n",
              " './',\n",
              " '.',\n",
              " 'Implementation/nn',\n",
              " 'of/in',\n",
              " \"Georgia's/np\",\n",
              " '$',\n",
              " 'automobile/nn',\n",
              " 'title/nn',\n",
              " 'law/nn',\n",
              " 'was/bedz',\n",
              " 'also/rb',\n",
              " 'recommended/vbn',\n",
              " 'by/in',\n",
              " 'the/at',\n",
              " 'outgoing/jj',\n",
              " 'jury/nn',\n",
              " './',\n",
              " '.',\n",
              " 'It/pps',\n",
              " 'urged/vbd',\n",
              " 'that/cs',\n",
              " 'the/at',\n",
              " 'next/ap',\n",
              " 'Legislature/nn-tl',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'provide/vb',\n",
              " 'enabling/vbg',\n",
              " 'funds/nns',\n",
              " 'and/cc',\n",
              " 're-set/vb',\n",
              " 'the/at',\n",
              " 'effective/jj',\n",
              " 'date/nn',\n",
              " 'so/cs',\n",
              " 'that/cs',\n",
              " 'an/at',\n",
              " 'orderly/jj',\n",
              " 'implementation/nn',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'law/nn',\n",
              " 'may/md',\n",
              " 'be/be',\n",
              " 'effected/vbn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'grand/jj',\n",
              " 'jury/nn',\n",
              " 'took/vbd',\n",
              " 'a/at',\n",
              " 'swipe/nn',\n",
              " 'at/in',\n",
              " 'the/at',\n",
              " 'State/nn-tl',\n",
              " 'Welfare/nn-tl',\n",
              " \"Department's/nn\",\n",
              " '$',\n",
              " '-tl',\n",
              " 'handling/nn',\n",
              " 'of/in',\n",
              " 'federal/jj',\n",
              " 'funds/nns',\n",
              " 'granted/vbn',\n",
              " 'for/in',\n",
              " 'child/nn',\n",
              " 'welfare/nn',\n",
              " 'services/nns',\n",
              " 'in/in',\n",
              " 'foster/jj',\n",
              " 'homes/nns',\n",
              " './',\n",
              " '.',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'This/dt',\n",
              " 'is/bez',\n",
              " 'one/cd',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'major/jj',\n",
              " 'items/nns',\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " 'general/jj',\n",
              " 'assistance/nn',\n",
              " 'program/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'but/cc',\n",
              " 'the/at',\n",
              " 'State/nn-tl',\n",
              " 'Welfare/nn-tl',\n",
              " 'Department/nn-tl',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'has/hvz',\n",
              " 'seen/vbn',\n",
              " 'fit/jj',\n",
              " 'to/to',\n",
              " 'distribute/vb',\n",
              " 'these/dts',\n",
              " 'funds/nns',\n",
              " 'through/in',\n",
              " 'the/at',\n",
              " 'welfare/nn',\n",
              " 'departments/nns',\n",
              " 'of/in',\n",
              " 'all/abn',\n",
              " 'the/at',\n",
              " 'counties/nns',\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'state/nn',\n",
              " 'with/in',\n",
              " 'the/at',\n",
              " 'exception/nn',\n",
              " 'of/in',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'which/wdt',\n",
              " 'receives/vbz',\n",
              " 'none/pn',\n",
              " 'of/in',\n",
              " 'this/dt',\n",
              " 'money/nn',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jurors/nns',\n",
              " 'said/vbd',\n",
              " 'they/ppss',\n",
              " 'realize/vb',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'a/at',\n",
              " 'proportionate/jj',\n",
              " 'distribution/nn',\n",
              " 'of/in',\n",
              " 'these/dts',\n",
              " 'funds/nns',\n",
              " 'might/md',\n",
              " 'disable/vb',\n",
              " 'this/dt',\n",
              " 'program/nn',\n",
              " 'in/in',\n",
              " 'our/pp',\n",
              " '$',\n",
              " 'less/ql',\n",
              " 'populous/jj',\n",
              " 'counties/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'Nevertheless/rb',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'we/ppss',\n",
              " 'feel/vb',\n",
              " 'that/cs',\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'future/nn',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " 'should/md',\n",
              " 'receive/vb',\n",
              " 'some/dti',\n",
              " 'portion/nn',\n",
              " 'of/in',\n",
              " 'these/dts',\n",
              " 'available/jj',\n",
              " 'funds/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jurors/nns',\n",
              " 'said/vbd',\n",
              " './',\n",
              " '.',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'Failure/nn',\n",
              " 'to/to',\n",
              " 'do/do',\n",
              " 'this/dt',\n",
              " 'will/md',\n",
              " 'continue/vb',\n",
              " 'to/to',\n",
              " 'place/vb',\n",
              " 'a/at',\n",
              " 'disproportionate/jj',\n",
              " 'burden/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'on/in',\n",
              " 'Fulton/np',\n",
              " 'taxpayers/nns',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'also/rb',\n",
              " 'commented/vbd',\n",
              " 'on/in',\n",
              " 'the/at',\n",
              " 'Fulton/np',\n",
              " \"ordinary's/nn\",\n",
              " '$',\n",
              " 'court/nn',\n",
              " 'which/wdt',\n",
              " 'has/hvz',\n",
              " 'been/ben',\n",
              " 'under/in',\n",
              " 'fire/nn',\n",
              " 'for/in',\n",
              " 'its/pp',\n",
              " '$',\n",
              " 'practices/nns',\n",
              " 'in/in',\n",
              " 'the/at',\n",
              " 'appointment/nn',\n",
              " 'of/in',\n",
              " 'appraisers/nns',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'guardians/nns',\n",
              " 'and/cc',\n",
              " 'administrators/nns',\n",
              " 'and/cc',\n",
              " 'the/at',\n",
              " 'awarding/nn',\n",
              " 'of/in',\n",
              " 'fees/nns',\n",
              " 'and/cc',\n",
              " 'compensation/nn',\n",
              " './',\n",
              " '.',\n",
              " 'Wards/nns-hl',\n",
              " 'protected/vbn-hl',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " 'it/pps',\n",
              " 'found/vbd',\n",
              " 'the/at',\n",
              " 'court/nn',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'has/hvz',\n",
              " 'incorporated/vbn',\n",
              " 'into/in',\n",
              " 'its/pp',\n",
              " '$',\n",
              " 'operating/vbg',\n",
              " 'procedures/nns',\n",
              " 'the/at',\n",
              " 'recommendations/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'of/in',\n",
              " 'two/cd',\n",
              " 'previous/jj',\n",
              " 'grand/jj',\n",
              " 'juries/nns',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'Atlanta/np-tl',\n",
              " 'Bar/nn-tl',\n",
              " 'Association/nn-tl',\n",
              " 'and/cc',\n",
              " 'an/at',\n",
              " 'interim/nn',\n",
              " 'citizens/nns',\n",
              " 'committee/nn',\n",
              " './',\n",
              " '.',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'These/dts',\n",
              " 'actions/nns',\n",
              " 'should/md',\n",
              " 'serve/vb',\n",
              " 'to/to',\n",
              " 'protect/vb',\n",
              " 'in/in',\n",
              " 'fact/nn',\n",
              " 'and/cc',\n",
              " 'in/in',\n",
              " 'effect/nn',\n",
              " 'the/at',\n",
              " \"court's/nn\",\n",
              " '$',\n",
              " 'wards/nns',\n",
              " 'from/in',\n",
              " 'undue/jj',\n",
              " 'costs/nns',\n",
              " 'and/cc',\n",
              " 'its/pp',\n",
              " '$',\n",
              " 'appointed/vbn',\n",
              " 'and/cc',\n",
              " 'elected/vbn',\n",
              " 'servants/nns',\n",
              " 'from/in',\n",
              " 'unmeritorious/jj',\n",
              " 'criticisms/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'said/vbd',\n",
              " './',\n",
              " '.',\n",
              " 'Regarding/in',\n",
              " \"Atlanta's/np\",\n",
              " '$',\n",
              " 'new/jj',\n",
              " 'multi-million-dollar/jj',\n",
              " 'airport/nn',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'recommended/vbd',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'that/cs',\n",
              " 'when/wrb',\n",
              " 'the/at',\n",
              " 'new/jj',\n",
              " 'management/nn',\n",
              " 'takes/vbz',\n",
              " 'charge/nn',\n",
              " 'Jan./np',\n",
              " '1/cd',\n",
              " 'the/at',\n",
              " 'airport/nn',\n",
              " 'be/be',\n",
              " 'operated/vbn',\n",
              " 'in/in',\n",
              " 'a/at',\n",
              " 'manner/nn',\n",
              " 'that/wps',\n",
              " 'will/md',\n",
              " 'eliminate/vb',\n",
              " 'political/jj',\n",
              " 'influences/nns',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'did/dod',\n",
              " 'not/',\n",
              " '*',\n",
              " 'elaborate/vb',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'but/cc',\n",
              " 'it/pps',\n",
              " 'added/vbd',\n",
              " 'that/cs',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'there/ex',\n",
              " 'should/md',\n",
              " 'be/be',\n",
              " 'periodic/jj',\n",
              " 'surveillance/nn',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'pricing/vbg',\n",
              " 'practices/nns',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'concessionaires/nns',\n",
              " 'for/in',\n",
              " 'the/at',\n",
              " 'purpose/nn',\n",
              " 'of/in',\n",
              " 'keeping/vbg',\n",
              " 'the/at',\n",
              " 'prices/nns',\n",
              " 'reasonable/jj',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " 'Ask/vb-hl',\n",
              " 'jail/nn-hl',\n",
              " 'deputies/nns-hl',\n",
              " 'On/in',\n",
              " 'other/ap',\n",
              " 'matters/nns',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'jury/nn',\n",
              " 'recommended/vbd',\n",
              " 'that/cs',\n",
              " ':',\n",
              " '/',\n",
              " ':',\n",
              " '(',\n",
              " '/',\n",
              " '(',\n",
              " '1/cd',\n",
              " ')',\n",
              " '/',\n",
              " ')',\n",
              " 'Four/cd',\n",
              " 'additional/jj',\n",
              " 'deputies/nns',\n",
              " 'be/be',\n",
              " 'employed/vbn',\n",
              " 'at/in',\n",
              " 'the/at',\n",
              " 'Fulton/np-tl',\n",
              " 'County/nn-tl',\n",
              " 'Jail/nn-tl',\n",
              " 'and/cc',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'a/at',\n",
              " 'doctor/nn',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'medical/jj',\n",
              " 'intern/nn',\n",
              " 'or/cc',\n",
              " 'extern/nn',\n",
              " 'be/be',\n",
              " 'employed/vbn',\n",
              " 'for/in',\n",
              " 'night/nn',\n",
              " 'and/cc',\n",
              " 'weekend/nn',\n",
              " 'duty/nn',\n",
              " 'at/in',\n",
              " 'the/at',\n",
              " 'jail/nn',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " './',\n",
              " '.',\n",
              " '(',\n",
              " '/',\n",
              " '(',\n",
              " '2/cd',\n",
              " ')',\n",
              " '/',\n",
              " ')',\n",
              " 'Fulton/np',\n",
              " 'legislators/nns',\n",
              " '``',\n",
              " '/',\n",
              " '``',\n",
              " 'work/vb',\n",
              " 'with/in',\n",
              " 'city/nn',\n",
              " 'officials/nns',\n",
              " 'to/to',\n",
              " 'pass/vb',\n",
              " 'enabling/vbg',\n",
              " 'legislation/nn',\n",
              " 'that/wps',\n",
              " 'will/md',\n",
              " 'permit/vb',\n",
              " 'the/at',\n",
              " 'establishment/nn',\n",
              " 'of/in',\n",
              " 'a/at',\n",
              " 'fair/jj',\n",
              " 'and/cc',\n",
              " 'equitable/jj',\n",
              " '``',\n",
              " '/',\n",
              " \"''\",\n",
              " 'pension/nn',\n",
              " 'plan/nn',\n",
              " 'for/in',\n",
              " 'city/nn',\n",
              " 'employes/nns',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'jury/nn',\n",
              " 'praised/vbd',\n",
              " 'the/at',\n",
              " 'administration/nn',\n",
              " 'and/cc',\n",
              " 'operation/nn',\n",
              " 'of/in',\n",
              " 'the/at',\n",
              " 'Atlanta/np-tl',\n",
              " 'Police/nns-tl',\n",
              " 'Department/nn-tl',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'Fulton/np-tl',\n",
              " 'Tax/nn-tl',\n",
              " \"Commissioner's/nn\",\n",
              " '$',\n",
              " '-tl',\n",
              " 'Office/nn-tl',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'the/at',\n",
              " 'Bellwood/np',\n",
              " 'and/cc',\n",
              " 'Alpharetta/np',\n",
              " 'prison/nn',\n",
              " 'farms/nns',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'Grady/np-tl',\n",
              " 'Hospital/nn-tl',\n",
              " 'and/cc',\n",
              " 'the/at',\n",
              " 'Fulton/np-tl',\n",
              " 'Health/nn-tl',\n",
              " 'Department/nn-tl',\n",
              " './',\n",
              " '.',\n",
              " 'Mayor/nn-tl',\n",
              " 'William/np',\n",
              " 'B./np',\n",
              " 'Hartsfield/np',\n",
              " 'filed/vbd',\n",
              " 'suit/nn',\n",
              " 'for/in',\n",
              " 'divorce/nn',\n",
              " 'from/in',\n",
              " 'his/pp',\n",
              " '$',\n",
              " 'wife/nn',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'Pearl/np',\n",
              " 'Williams/np',\n",
              " 'Hartsfield/np',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " 'in/in',\n",
              " 'Fulton/np-tl',\n",
              " 'Superior/jj-tl',\n",
              " 'Court/nn-tl',\n",
              " 'Friday/nr',\n",
              " './',\n",
              " '.',\n",
              " 'His/pp',\n",
              " '$',\n",
              " 'petition/nn',\n",
              " 'charged/vbd',\n",
              " 'mental/jj',\n",
              " 'cruelty/nn',\n",
              " './',\n",
              " '.',\n",
              " 'The/at',\n",
              " 'couple/nn',\n",
              " 'was/bedz',\n",
              " 'married/vbn',\n",
              " 'Aug./np',\n",
              " '2/cd',\n",
              " ',',\n",
              " '/',\n",
              " ',',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categories in brown corpua\n",
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zphz9RSEB9uG",
        "outputId": "37cbcdc2-6913-48b4-dd3b-c2544a4e2038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the size of word tokens\n",
        "print(\"Size of word tokens:\",len(brown.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yKe93LxUnVi",
        "outputId": "473fb87a-230e-4cfc-96d9-c3fe17b4ba28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word tokens: 1161192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the size of word types\n",
        "word_types = len(set(brown.words()))\n",
        "print(\"Size of word types:\", word_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HvEMgg_cw0V",
        "outputId": "208d5241-3660-4bea-92e3-53bd15ff4bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the size of the category \"government\"\n",
        "print(\"Size of the category 'government':\", len(brown.words(categories='government')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy0mQoymCqfL",
        "outputId": "02931346-7998-4cdf-e3c1-aafb6ab10815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the category 'government': 70117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the most frequent tokens\n",
        "from nltk import FreqDist\n",
        "fdist1 = FreqDist(brown.words())\n",
        "fdist1.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj9wZF31GHpF",
        "outputId": "6177b247-5304-4cd7-876b-e744a7e64725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 62713),\n",
              " (',', 58334),\n",
              " ('.', 49346),\n",
              " ('of', 36080),\n",
              " ('and', 27915),\n",
              " ('to', 25732),\n",
              " ('a', 21881),\n",
              " ('in', 19536),\n",
              " ('that', 10237),\n",
              " ('is', 10011)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of sentences\n",
        "print(\"Number of sentences:\", len(brown.sents()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqyXkbfQEToR",
        "outputId": "74b5623e-c6d9-404d-b9ec-81c800632307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 57340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "I0NeXljqfWFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explore the corpora available in NLTK (any two) \n",
        "\n",
        "• Raw corpus\n",
        "\n",
        "• POS tagged\n",
        "\n",
        "• Parsed\n",
        "\n",
        "• Multilingual aligned\n",
        "\n",
        "• Spoken language\n",
        "\n",
        "• Semantic tagged"
      ],
      "metadata": {
        "id": "TKzlT78XduPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Corpus"
      ],
      "metadata": {
        "id": "Hd4YrgdHe5Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA2IJV1ld7ob",
        "outputId": "ccd6bdc5-cb79-43ae-af59-2db9cea857a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tju1czDdzLa",
        "outputId": "ff9f9d07-3321-4212-ed75-5d8416c3529c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of words : \", len(gutenberg.words()))\n",
        "print(\"Number of sentences : \", len(gutenberg.sents()))\n",
        "print(\"Number of paragraphs : \", len(gutenberg.paras()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt74Oxe0eAHO",
        "outputId": "ae628555-a348-46ae-fd3b-ebbcdefdad46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words :  2621613\n",
            "Number of sentences :  98503\n",
            "Number of paragraphs :  47887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebThg_UleCKF",
        "outputId": "74355d3c-4af3-4a5f-a4b8-bd50e3ee798b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2bMjSWxeRNZ",
        "outputId": "dee00ea5-454a-42cb-ca33-5fafa547f48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.paras()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ykepCOReUTQ",
        "outputId": "260ac127-8acd-43b9-8468-886da921d002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_types = len(set(gutenberg.words()))\n",
        "print(\"Size of word types:\", word_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ72Zy9JebmQ",
        "outputId": "dae4e346-c388-48ad-f86d-d654afbe6361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word types: 51156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
        "for fileid in gutenberg.fileids(): \n",
        "    num_chars = len(gutenberg.raw(fileid))\n",
        "    num_words = len(gutenberg.words(fileid))\n",
        "    num_sents = len(gutenberg.sents(fileid))\n",
        "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZqTtsZmehwn",
        "outputId": "389e420e-454e-4175-d707-bfb431e587f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
            "5               25               26        austen-emma.txt\n",
            "5               26               17        austen-persuasion.txt\n",
            "5               28               22        austen-sense.txt\n",
            "4               34               79        bible-kjv.txt\n",
            "5               19               5        blake-poems.txt\n",
            "4               19               14        bryant-stories.txt\n",
            "4               18               12        burgess-busterbrown.txt\n",
            "4               20               13        carroll-alice.txt\n",
            "5               20               12        chesterton-ball.txt\n",
            "5               23               11        chesterton-brown.txt\n",
            "5               19               11        chesterton-thursday.txt\n",
            "4               21               25        edgeworth-parents.txt\n",
            "5               26               15        melville-moby_dick.txt\n",
            "5               52               11        milton-paradise.txt\n",
            "4               12               9        shakespeare-caesar.txt\n",
            "4               12               8        shakespeare-hamlet.txt\n",
            "4               12               7        shakespeare-macbeth.txt\n",
            "5               36               12        whitman-leaves.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsed Corpus"
      ],
      "metadata": {
        "id": "-h1DFAUJez7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0czF-pefKOn",
        "outputId": "b9619a43-1d96-4018-b030-ce456094ef41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "treebank.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3KMILpLezXX",
        "outputId": "16f13fda-5e44-4355-f219-aaf9f3acc054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.tagged_words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPSOCu2WfNL2",
        "outputId": "9245c00e-7e92-4e76-aba7-bb9bb82b5d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbO8iK4ufPiO",
        "outputId": "c3e90cd9-641c-4d49-be87-5c1bc78bd170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.tagged_sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I27XiKMwfSK_",
        "outputId": "57307540-473e-4689-adbb-a31bea2202c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = nltk.FreqDist(treebank.words())\n",
        "top_tokens = fdist.most_common(10)\n",
        "print(\"Most frequent tokens:\", top_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_56F37vfTiO",
        "outputId": "63165a22-1ffd-4d3a-a9e0-086e72293c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent tokens: [(',', 4885), ('the', 4045), ('.', 3828), ('of', 2319), ('to', 2164), ('a', 1878), ('in', 1572), ('and', 1511), ('*-1', 1123), ('0', 1099)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of files in treebank corpus : \",len(treebank.fileids()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fr6BSF_fWZ2",
        "outputId": "f98b0c0c-8e9a-4932-9389-ab475fc63745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in treebank corpus :  199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg.Word.Len \",\"Avg.Sent.Len \",\"Avg.Wrd.Freq \",\"FileName \")\n",
        "for fileid in treebank.fileids(): \n",
        "    num_chars = len(treebank.raw(fileid))\n",
        "    num_words = len(treebank.words(fileid))\n",
        "    num_sents = len(treebank.sents(fileid))\n",
        "    num_vocab = len(set(w.lower() for w in treebank.words(fileid)))\n",
        "    print(round(num_chars/num_words),\"             \", round(num_words/num_sents), \"             \", round(num_words/num_vocab), \"      \", fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKCfGaP-fcHu",
        "outputId": "b2a6c52d-a803-44f5-bd8e-6f63c9444a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg.Word.Len  Avg.Sent.Len  Avg.Wrd.Freq  FileName \n",
            "21               16               1        wsj_0001.mrg\n",
            "23               27               1        wsj_0002.mrg\n",
            "29               26               2        wsj_0003.mrg\n",
            "24               24               2        wsj_0004.mrg\n",
            "25               19               1        wsj_0005.mrg\n",
            "28               24               1        wsj_0006.mrg\n",
            "23               19               1        wsj_0007.mrg\n",
            "26               23               2        wsj_0008.mrg\n",
            "23               20               2        wsj_0009.mrg\n",
            "24               22               2        wsj_0010.mrg\n",
            "24               26               2        wsj_0011.mrg\n",
            "26               28               2        wsj_0012.mrg\n",
            "28               31               2        wsj_0013.mrg\n",
            "22               37               1        wsj_0014.mrg\n",
            "26               26               2        wsj_0015.mrg\n",
            "25               22               2        wsj_0016.mrg\n",
            "28               26               2        wsj_0017.mrg\n",
            "27               27               2        wsj_0018.mrg\n",
            "24               15               2        wsj_0019.mrg\n",
            "30               31               2        wsj_0020.mrg\n",
            "29               25               2        wsj_0021.mrg\n",
            "25               21               2        wsj_0022.mrg\n",
            "25               24               1        wsj_0023.mrg\n",
            "29               22               2        wsj_0024.mrg\n",
            "26               29               2        wsj_0025.mrg\n",
            "30               32               2        wsj_0026.mrg\n",
            "27               23               2        wsj_0027.mrg\n",
            "22               23               1        wsj_0028.mrg\n",
            "24               27               2        wsj_0029.mrg\n",
            "23               27               1        wsj_0030.mrg\n",
            "32               40               1        wsj_0031.mrg\n",
            "26               24               2        wsj_0032.mrg\n",
            "24               24               2        wsj_0033.mrg\n",
            "27               24               2        wsj_0034.mrg\n",
            "30               28               2        wsj_0035.mrg\n",
            "27               21               3        wsj_0036.mrg\n",
            "28               28               2        wsj_0037.mrg\n",
            "30               24               2        wsj_0038.mrg\n",
            "27               25               2        wsj_0039.mrg\n",
            "25               27               2        wsj_0040.mrg\n",
            "27               25               3        wsj_0041.mrg\n",
            "23               24               2        wsj_0042.mrg\n",
            "25               26               3        wsj_0043.mrg\n",
            "28               23               3        wsj_0044.mrg\n",
            "28               27               3        wsj_0045.mrg\n",
            "23               19               1        wsj_0046.mrg\n",
            "31               29               2        wsj_0047.mrg\n",
            "27               25               2        wsj_0048.mrg\n",
            "29               27               3        wsj_0049.mrg\n",
            "31               21               1        wsj_0050.mrg\n",
            "28               25               2        wsj_0051.mrg\n",
            "25               15               1        wsj_0052.mrg\n",
            "25               30               2        wsj_0053.mrg\n",
            "28               23               1        wsj_0054.mrg\n",
            "29               30               1        wsj_0055.mrg\n",
            "19               9               1        wsj_0056.mrg\n",
            "26               25               2        wsj_0057.mrg\n",
            "27               25               1        wsj_0058.mrg\n",
            "27               32               2        wsj_0059.mrg\n",
            "29               27               2        wsj_0060.mrg\n",
            "25               18               1        wsj_0061.mrg\n",
            "28               23               2        wsj_0062.mrg\n",
            "29               30               2        wsj_0063.mrg\n",
            "28               25               2        wsj_0064.mrg\n",
            "27               24               1        wsj_0065.mrg\n",
            "22               24               1        wsj_0066.mrg\n",
            "30               32               2        wsj_0067.mrg\n",
            "23               19               2        wsj_0068.mrg\n",
            "25               23               1        wsj_0069.mrg\n",
            "31               33               1        wsj_0070.mrg\n",
            "27               29               3        wsj_0071.mrg\n",
            "27               29               2        wsj_0072.mrg\n",
            "29               26               2        wsj_0073.mrg\n",
            "26               18               2        wsj_0074.mrg\n",
            "29               30               2        wsj_0075.mrg\n",
            "25               18               1        wsj_0076.mrg\n",
            "26               17               2        wsj_0077.mrg\n",
            "24               28               1        wsj_0078.mrg\n",
            "24               16               1        wsj_0079.mrg\n",
            "29               28               2        wsj_0080.mrg\n",
            "28               32               2        wsj_0081.mrg\n",
            "28               26               2        wsj_0082.mrg\n",
            "25               26               3        wsj_0083.mrg\n",
            "24               21               2        wsj_0084.mrg\n",
            "27               24               2        wsj_0085.mrg\n",
            "32               22               2        wsj_0086.mrg\n",
            "26               18               2        wsj_0087.mrg\n",
            "30               32               3        wsj_0088.mrg\n",
            "27               27               3        wsj_0089.mrg\n",
            "26               27               3        wsj_0090.mrg\n",
            "30               28               2        wsj_0091.mrg\n",
            "29               27               2        wsj_0092.mrg\n",
            "28               27               2        wsj_0093.mrg\n",
            "28               22               2        wsj_0094.mrg\n",
            "28               26               2        wsj_0095.mrg\n",
            "25               38               3        wsj_0096.mrg\n",
            "28               26               2        wsj_0097.mrg\n",
            "28               33               2        wsj_0098.mrg\n",
            "22               22               2        wsj_0099.mrg\n",
            "27               23               2        wsj_0100.mrg\n",
            "27               42               2        wsj_0101.mrg\n",
            "27               20               2        wsj_0102.mrg\n",
            "29               29               2        wsj_0103.mrg\n",
            "20               16               1        wsj_0104.mrg\n",
            "29               28               2        wsj_0105.mrg\n",
            "27               25               2        wsj_0106.mrg\n",
            "34               40               2        wsj_0107.mrg\n",
            "29               23               3        wsj_0108.mrg\n",
            "28               25               2        wsj_0109.mrg\n",
            "20               22               3        wsj_0110.mrg\n",
            "29               25               2        wsj_0111.mrg\n",
            "31               31               3        wsj_0112.mrg\n",
            "25               26               2        wsj_0113.mrg\n",
            "26               24               2        wsj_0114.mrg\n",
            "30               28               2        wsj_0115.mrg\n",
            "27               28               3        wsj_0116.mrg\n",
            "28               27               2        wsj_0117.mrg\n",
            "27               26               4        wsj_0118.mrg\n",
            "27               31               2        wsj_0119.mrg\n",
            "25               21               2        wsj_0120.mrg\n",
            "25               23               3        wsj_0121.mrg\n",
            "30               30               1        wsj_0122.mrg\n",
            "30               26               2        wsj_0123.mrg\n",
            "25               23               2        wsj_0124.mrg\n",
            "23               20               3        wsj_0125.mrg\n",
            "27               22               2        wsj_0126.mrg\n",
            "25               28               2        wsj_0127.mrg\n",
            "27               28               3        wsj_0128.mrg\n",
            "30               31               2        wsj_0129.mrg\n",
            "31               28               2        wsj_0130.mrg\n",
            "22               27               1        wsj_0131.mrg\n",
            "26               27               2        wsj_0132.mrg\n",
            "28               25               1        wsj_0133.mrg\n",
            "29               26               2        wsj_0134.mrg\n",
            "28               25               2        wsj_0135.mrg\n",
            "26               30               2        wsj_0136.mrg\n",
            "27               21               2        wsj_0137.mrg\n",
            "21               26               2        wsj_0138.mrg\n",
            "26               16               1        wsj_0139.mrg\n",
            "26               29               1        wsj_0140.mrg\n",
            "24               23               2        wsj_0141.mrg\n",
            "25               24               3        wsj_0142.mrg\n",
            "24               32               1        wsj_0143.mrg\n",
            "24               29               2        wsj_0144.mrg\n",
            "26               27               2        wsj_0145.mrg\n",
            "31               30               2        wsj_0146.mrg\n",
            "27               32               2        wsj_0147.mrg\n",
            "26               29               3        wsj_0148.mrg\n",
            "29               26               2        wsj_0149.mrg\n",
            "25               20               2        wsj_0150.mrg\n",
            "29               31               2        wsj_0151.mrg\n",
            "24               23               2        wsj_0152.mrg\n",
            "24               26               2        wsj_0153.mrg\n",
            "26               29               2        wsj_0154.mrg\n",
            "29               26               3        wsj_0155.mrg\n",
            "32               29               2        wsj_0156.mrg\n",
            "25               29               2        wsj_0157.mrg\n",
            "26               25               2        wsj_0158.mrg\n",
            "28               25               2        wsj_0159.mrg\n",
            "27               32               2        wsj_0160.mrg\n",
            "30               32               2        wsj_0161.mrg\n",
            "27               25               2        wsj_0162.mrg\n",
            "28               25               2        wsj_0163.mrg\n",
            "27               27               2        wsj_0164.mrg\n",
            "27               29               2        wsj_0165.mrg\n",
            "28               32               2        wsj_0166.mrg\n",
            "25               24               2        wsj_0167.mrg\n",
            "24               30               2        wsj_0168.mrg\n",
            "24               25               2        wsj_0169.mrg\n",
            "29               22               2        wsj_0170.mrg\n",
            "26               23               2        wsj_0171.mrg\n",
            "28               26               2        wsj_0172.mrg\n",
            "26               29               2        wsj_0173.mrg\n",
            "29               25               2        wsj_0174.mrg\n",
            "26               24               2        wsj_0175.mrg\n",
            "28               33               2        wsj_0176.mrg\n",
            "26               21               2        wsj_0177.mrg\n",
            "25               20               2        wsj_0178.mrg\n",
            "26               23               2        wsj_0179.mrg\n",
            "28               26               2        wsj_0180.mrg\n",
            "27               30               2        wsj_0181.mrg\n",
            "26               24               2        wsj_0182.mrg\n",
            "26               29               2        wsj_0183.mrg\n",
            "28               24               2        wsj_0184.mrg\n",
            "23               13               1        wsj_0185.mrg\n",
            "28               26               2        wsj_0186.mrg\n",
            "26               26               2        wsj_0187.mrg\n",
            "28               30               2        wsj_0188.mrg\n",
            "23               25               2        wsj_0189.mrg\n",
            "25               17               1        wsj_0190.mrg\n",
            "26               27               1        wsj_0191.mrg\n",
            "27               25               3        wsj_0192.mrg\n",
            "28               26               1        wsj_0193.mrg\n",
            "27               28               2        wsj_0194.mrg\n",
            "28               31               1        wsj_0195.mrg\n",
            "25               44               1        wsj_0196.mrg\n",
            "29               54               1        wsj_0197.mrg\n",
            "29               32               2        wsj_0198.mrg\n",
            "28               15               1        wsj_0199.mrg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4iUdZapxfz5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a text corpus with a minimum of 200 words (unique content). Implement the following text processing\n",
        "\n",
        "• Word segmentation\n",
        "\n",
        "• Sentence segmentation\n",
        "\n",
        "• Convert to Lowercase\n",
        "\n",
        "• Stop words removal\n",
        "\n",
        "• Stemming\n",
        "\n",
        "• Lemmatization\n",
        "\n",
        "• Part of speech tagger\n"
      ],
      "metadata": {
        "id": "sTFtQT4Afp12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Segmentation"
      ],
      "metadata": {
        "id": "Oju1xbyXfzh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Automatic Multiple Choice Question (MCQ) generation from a text is a popular research area. MCQs are widely accepted for large-scale assessment in various domains and applications. However, manual generation of MCQs is expensive and time-consuming. Therefore, researchers were attracted towards automatic MCQ generation since the late 90’s. Since then, many systems have been developed for MCQ generation. After key selection, the next task becomes a transformation of the declarative sentence into the interrogative form. How\u0002ever, in the literature, we found that this step has been ignored in many MCQ systems. If the transformation is not done, then the sentence remains in its original form, and a blank replaces the key. As a result, it becomes a fill\u0002in-the-blank type question with distractors. However, we found several works containing the transformation from the declarative to an interrogative sentence.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "233UfTV7JoE4",
        "outputId": "b81a021d-d23d-4742-c59f-a62585104603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Automatic',\n",
              " 'Multiple',\n",
              " 'Choice',\n",
              " 'Question',\n",
              " '(',\n",
              " 'MCQ',\n",
              " ')',\n",
              " 'generation',\n",
              " 'from',\n",
              " 'a',\n",
              " 'text',\n",
              " 'is',\n",
              " 'a',\n",
              " 'popular',\n",
              " 'research',\n",
              " 'area',\n",
              " '.',\n",
              " 'MCQs',\n",
              " 'are',\n",
              " 'widely',\n",
              " 'accepted',\n",
              " 'for',\n",
              " 'large-scale',\n",
              " 'assessment',\n",
              " 'in',\n",
              " 'various',\n",
              " 'domains',\n",
              " 'and',\n",
              " 'applications',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'manual',\n",
              " 'generation',\n",
              " 'of',\n",
              " 'MCQs',\n",
              " 'is',\n",
              " 'expensive',\n",
              " 'and',\n",
              " 'time-consuming',\n",
              " '.',\n",
              " 'Therefore',\n",
              " ',',\n",
              " 'researchers',\n",
              " 'were',\n",
              " 'attracted',\n",
              " 'towards',\n",
              " 'automatic',\n",
              " 'MCQ',\n",
              " 'generation',\n",
              " 'since',\n",
              " 'the',\n",
              " 'late',\n",
              " '90',\n",
              " '’',\n",
              " 's',\n",
              " '.',\n",
              " 'Since',\n",
              " 'then',\n",
              " ',',\n",
              " 'many',\n",
              " 'systems',\n",
              " 'have',\n",
              " 'been',\n",
              " 'developed',\n",
              " 'for',\n",
              " 'MCQ',\n",
              " 'generation',\n",
              " '.',\n",
              " 'After',\n",
              " 'key',\n",
              " 'selection',\n",
              " ',',\n",
              " 'the',\n",
              " 'next',\n",
              " 'task',\n",
              " 'becomes',\n",
              " 'a',\n",
              " 'transformation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'declarative',\n",
              " 'sentence',\n",
              " 'into',\n",
              " 'the',\n",
              " 'interrogative',\n",
              " 'form',\n",
              " '.',\n",
              " 'How\\x02ever',\n",
              " ',',\n",
              " 'in',\n",
              " 'the',\n",
              " 'literature',\n",
              " ',',\n",
              " 'we',\n",
              " 'found',\n",
              " 'that',\n",
              " 'this',\n",
              " 'step',\n",
              " 'has',\n",
              " 'been',\n",
              " 'ignored',\n",
              " 'in',\n",
              " 'many',\n",
              " 'MCQ',\n",
              " 'systems',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'transformation',\n",
              " 'is',\n",
              " 'not',\n",
              " 'done',\n",
              " ',',\n",
              " 'then',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'remains',\n",
              " 'in',\n",
              " 'its',\n",
              " 'original',\n",
              " 'form',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'blank',\n",
              " 'replaces',\n",
              " 'the',\n",
              " 'key',\n",
              " '.',\n",
              " 'As',\n",
              " 'a',\n",
              " 'result',\n",
              " ',',\n",
              " 'it',\n",
              " 'becomes',\n",
              " 'a',\n",
              " 'fill\\x02in-the-blank',\n",
              " 'type',\n",
              " 'question',\n",
              " 'with',\n",
              " 'distractors',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'we',\n",
              " 'found',\n",
              " 'several',\n",
              " 'works',\n",
              " 'containing',\n",
              " 'the',\n",
              " 'transformation',\n",
              " 'from',\n",
              " 'the',\n",
              " 'declarative',\n",
              " 'to',\n",
              " 'an',\n",
              " 'interrogative',\n",
              " 'sentence',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence segmentation "
      ],
      "metadata": {
        "id": "ruhcKuuMgCXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sents = sent_tokenizer.tokenize(text)\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pb58l58KUDp",
        "outputId": "52331b9c-3d86-4d2c-873d-f09ce3ed6003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Automatic Multiple Choice Question (MCQ) generation from a text is a popular research area.',\n",
              " 'MCQs are widely accepted for large-scale assessment in various domains and applications.',\n",
              " 'However, manual generation of MCQs is expensive and time-consuming.',\n",
              " 'Therefore, researchers were attracted towards automatic MCQ generation since the late 90’s.',\n",
              " 'Since then, many systems have been developed for MCQ generation.',\n",
              " 'After key selection, the next task becomes a transformation of the declarative sentence into the interrogative form.',\n",
              " 'How\\x02ever, in the literature, we found that this step has been ignored in many MCQ systems.',\n",
              " 'If the transformation is not done, then the sentence remains in its original form, and a blank replaces the key.',\n",
              " 'As a result, it becomes a fill\\x02in-the-blank type question with distractors.',\n",
              " 'However, we found several works containing the transformation from the declarative to an interrogative sentence.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to lowercase\n"
      ],
      "metadata": {
        "id": "HU_pivG7iRmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_text = text.lower()\n",
        "print (lower_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzqcSwxK7yr",
        "outputId": "7773f3e7-c4c1-452d-98b5-44f4be6cc8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "automatic multiple choice question (mcq) generation from a text is a popular research area. mcqs are widely accepted for large-scale assessment in various domains and applications. however, manual generation of mcqs is expensive and time-consuming. therefore, researchers were attracted towards automatic mcq generation since the late 90’s. since then, many systems have been developed for mcq generation. after key selection, the next task becomes a transformation of the declarative sentence into the interrogative form. how\u0002ever, in the literature, we found that this step has been ignored in many mcq systems. if the transformation is not done, then the sentence remains in its original form, and a blank replaces the key. as a result, it becomes a fill\u0002in-the-blank type question with distractors. however, we found several works containing the transformation from the declarative to an interrogative sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stop words removal"
      ],
      "metadata": {
        "id": "aumObctcifDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctqzTFNzM3KA",
        "outputId": "8d7e70e3-ba19-4d78-ce28-3dacbf9ec750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "word_tokens = word_tokenize(text.lower())\n",
        "filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
        "print(\"Words left after stop words removal : \",filtered_sentence) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vp4LySoMEfS",
        "outputId": "62aa910e-9cdc-493c-f670-25693e221714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words left after stop words removal :  ['automatic', 'multiple', 'choice', 'question', '(', 'mcq', ')', 'generation', 'text', 'popular', 'research', 'area', '.', 'mcqs', 'widely', 'accepted', 'large-scale', 'assessment', 'various', 'domains', 'applications', '.', 'however', ',', 'manual', 'generation', 'mcqs', 'expensive', 'time-consuming', '.', 'therefore', ',', 'researchers', 'attracted', 'towards', 'automatic', 'mcq', 'generation', 'since', 'late', '90', '’', '.', 'since', ',', 'many', 'systems', 'developed', 'mcq', 'generation', '.', 'key', 'selection', ',', 'next', 'task', 'becomes', 'transformation', 'declarative', 'sentence', 'interrogative', 'form', '.', 'how\\x02ever', ',', 'literature', ',', 'found', 'step', 'ignored', 'many', 'mcq', 'systems', '.', 'transformation', 'done', ',', 'sentence', 'remains', 'original', 'form', ',', 'blank', 'replaces', 'key', '.', 'result', ',', 'becomes', 'fill\\x02in-the-blank', 'type', 'question', 'distractors', '.', 'however', ',', 'found', 'several', 'works', 'containing', 'transformation', 'declarative', 'interrogative', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "vJGf2eNsi3bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in tokens]\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag0J2TFqjFVO",
        "outputId": "11645119-ae5a-4f21-dd3d-0b2ca34967a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['automat', 'multipl', 'choic', 'question', '(', 'mcq', ')', 'gener', 'from', 'a', 'text', 'is', 'a', 'popular', 'research', 'area', '.', 'mcq', 'are', 'wide', 'accept', 'for', 'large-scal', 'assess', 'in', 'variou', 'domain', 'and', 'applic', '.', 'howev', ',', 'manual', 'gener', 'of', 'mcq', 'is', 'expens', 'and', 'time-consum', '.', 'therefor', ',', 'research', 'were', 'attract', 'toward', 'automat', 'mcq', 'gener', 'sinc', 'the', 'late', '90', '’', 's', '.', 'sinc', 'then', ',', 'mani', 'system', 'have', 'been', 'develop', 'for', 'mcq', 'gener', '.', 'after', 'key', 'select', ',', 'the', 'next', 'task', 'becom', 'a', 'transform', 'of', 'the', 'declar', 'sentenc', 'into', 'the', 'interrog', 'form', '.', 'how\\x02ev', ',', 'in', 'the', 'literatur', ',', 'we', 'found', 'that', 'thi', 'step', 'ha', 'been', 'ignor', 'in', 'mani', 'mcq', 'system', '.', 'if', 'the', 'transform', 'is', 'not', 'done', ',', 'then', 'the', 'sentenc', 'remain', 'in', 'it', 'origin', 'form', ',', 'and', 'a', 'blank', 'replac', 'the', 'key', '.', 'as', 'a', 'result', ',', 'it', 'becom', 'a', 'fill\\x02in-the-blank', 'type', 'question', 'with', 'distractor', '.', 'howev', ',', 'we', 'found', 'sever', 'work', 'contain', 'the', 'transform', 'from', 'the', 'declar', 'to', 'an', 'interrog', 'sentenc', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "EWwOOm9Hi8KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcozpYevkQOC",
        "outputId": "b3d98fbd-59e8-4dbe-a6ff-ebf96a408e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "lemmatized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWTt_GvyjrKy",
        "outputId": "21ee9530-039c-451e-9e3b-d46473913ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Automatic',\n",
              " 'Multiple',\n",
              " 'Choice',\n",
              " 'Question',\n",
              " '(',\n",
              " 'MCQ',\n",
              " ')',\n",
              " 'generation',\n",
              " 'from',\n",
              " 'a',\n",
              " 'text',\n",
              " 'is',\n",
              " 'a',\n",
              " 'popular',\n",
              " 'research',\n",
              " 'area',\n",
              " '.',\n",
              " 'MCQs',\n",
              " 'are',\n",
              " 'widely',\n",
              " 'accepted',\n",
              " 'for',\n",
              " 'large-scale',\n",
              " 'assessment',\n",
              " 'in',\n",
              " 'various',\n",
              " 'domain',\n",
              " 'and',\n",
              " 'application',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'manual',\n",
              " 'generation',\n",
              " 'of',\n",
              " 'MCQs',\n",
              " 'is',\n",
              " 'expensive',\n",
              " 'and',\n",
              " 'time-consuming',\n",
              " '.',\n",
              " 'Therefore',\n",
              " ',',\n",
              " 'researcher',\n",
              " 'were',\n",
              " 'attracted',\n",
              " 'towards',\n",
              " 'automatic',\n",
              " 'MCQ',\n",
              " 'generation',\n",
              " 'since',\n",
              " 'the',\n",
              " 'late',\n",
              " '90',\n",
              " '’',\n",
              " 's',\n",
              " '.',\n",
              " 'Since',\n",
              " 'then',\n",
              " ',',\n",
              " 'many',\n",
              " 'system',\n",
              " 'have',\n",
              " 'been',\n",
              " 'developed',\n",
              " 'for',\n",
              " 'MCQ',\n",
              " 'generation',\n",
              " '.',\n",
              " 'After',\n",
              " 'key',\n",
              " 'selection',\n",
              " ',',\n",
              " 'the',\n",
              " 'next',\n",
              " 'task',\n",
              " 'becomes',\n",
              " 'a',\n",
              " 'transformation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'declarative',\n",
              " 'sentence',\n",
              " 'into',\n",
              " 'the',\n",
              " 'interrogative',\n",
              " 'form',\n",
              " '.',\n",
              " 'How\\x02ever',\n",
              " ',',\n",
              " 'in',\n",
              " 'the',\n",
              " 'literature',\n",
              " ',',\n",
              " 'we',\n",
              " 'found',\n",
              " 'that',\n",
              " 'this',\n",
              " 'step',\n",
              " 'ha',\n",
              " 'been',\n",
              " 'ignored',\n",
              " 'in',\n",
              " 'many',\n",
              " 'MCQ',\n",
              " 'system',\n",
              " '.',\n",
              " 'If',\n",
              " 'the',\n",
              " 'transformation',\n",
              " 'is',\n",
              " 'not',\n",
              " 'done',\n",
              " ',',\n",
              " 'then',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'remains',\n",
              " 'in',\n",
              " 'it',\n",
              " 'original',\n",
              " 'form',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'blank',\n",
              " 'replaces',\n",
              " 'the',\n",
              " 'key',\n",
              " '.',\n",
              " 'As',\n",
              " 'a',\n",
              " 'result',\n",
              " ',',\n",
              " 'it',\n",
              " 'becomes',\n",
              " 'a',\n",
              " 'fill\\x02in-the-blank',\n",
              " 'type',\n",
              " 'question',\n",
              " 'with',\n",
              " 'distractors',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'we',\n",
              " 'found',\n",
              " 'several',\n",
              " 'work',\n",
              " 'containing',\n",
              " 'the',\n",
              " 'transformation',\n",
              " 'from',\n",
              " 'the',\n",
              " 'declarative',\n",
              " 'to',\n",
              " 'an',\n",
              " 'interrogative',\n",
              " 'sentence',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tagging"
      ],
      "metadata": {
        "id": "1CXCilKlkSER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdUPSYvql_uz",
        "outputId": "978a5d26-1cfb-4cd3-9229-9e927e9e7318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized = sent_tokenize(text)\n",
        "for i in tokenized:\n",
        "\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        " \n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5p2SPVOlev_",
        "outputId": "724fdfc7-30e9-4821-e7ac-4648f9131acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Automatic', 'JJ'), ('Multiple', 'NNP'), ('Choice', 'NNP'), ('Question', 'NNP'), ('(', '('), ('MCQ', 'NNP'), (')', ')'), ('generation', 'NN'), ('text', 'IN'), ('popular', 'JJ'), ('research', 'NN'), ('area', 'NN'), ('.', '.')]\n",
            "[('MCQs', 'NNP'), ('widely', 'RB'), ('accepted', 'VBD'), ('large-scale', 'JJ'), ('assessment', 'NN'), ('various', 'JJ'), ('domains', 'NNS'), ('applications', 'NNS'), ('.', '.')]\n",
            "[('However', 'RB'), (',', ','), ('manual', 'JJ'), ('generation', 'NN'), ('MCQs', 'NNP'), ('expensive', 'JJ'), ('time-consuming', 'NN'), ('.', '.')]\n",
            "[('Therefore', 'RB'), (',', ','), ('researchers', 'NNS'), ('attracted', 'VBD'), ('towards', 'NNS'), ('automatic', 'JJ'), ('MCQ', 'NNP'), ('generation', 'NN'), ('since', 'IN'), ('late', 'JJ'), ('90', 'CD'), ('’', 'NN'), ('.', '.')]\n",
            "[('Since', 'IN'), (',', ','), ('many', 'JJ'), ('systems', 'NNS'), ('developed', 'VBD'), ('MCQ', 'NNP'), ('generation', 'NN'), ('.', '.')]\n",
            "[('After', 'IN'), ('key', 'JJ'), ('selection', 'NN'), (',', ','), ('next', 'JJ'), ('task', 'NN'), ('becomes', 'VBZ'), ('transformation', 'JJ'), ('declarative', 'JJ'), ('sentence', 'NN'), ('interrogative', 'JJ'), ('form', 'NN'), ('.', '.')]\n",
            "[('How\\x02ever', 'NNP'), (',', ','), ('literature', 'NN'), (',', ','), ('found', 'VBN'), ('step', 'NN'), ('ignored', 'VBD'), ('many', 'JJ'), ('MCQ', 'NNP'), ('systems', 'NNS'), ('.', '.')]\n",
            "[('If', 'IN'), ('transformation', 'NN'), ('done', 'VBN'), (',', ','), ('sentence', 'NN'), ('remains', 'VBZ'), ('original', 'JJ'), ('form', 'NN'), (',', ','), ('blank', 'JJ'), ('replaces', 'NNS'), ('key', 'NN'), ('.', '.')]\n",
            "[('As', 'IN'), ('result', 'NN'), (',', ','), ('becomes', 'VBZ'), ('fill\\x02in-the-blank', 'JJ'), ('type', 'JJ'), ('question', 'NN'), ('distractors', 'NNS'), ('.', '.')]\n",
            "[('However', 'RB'), (',', ','), ('found', 'VBD'), ('several', 'JJ'), ('works', 'NNS'), ('containing', 'VBG'), ('transformation', 'NN'), ('declarative', 'JJ'), ('interrogative', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}